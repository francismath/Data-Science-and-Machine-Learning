{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sdn1zR4nrutt"
      },
      "outputs": [],
      "source": [
        "# Train WGAN using MNIST\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, Dense, ReLU, LeakyReLU, BatchNormalization\n",
        "from keras.layers import Conv2D, Conv2DTranspose, Reshape, Flatten\n",
        "from keras.optimizers import RMSprop\n",
        "from keras import initializers\n",
        "from keras import backend as K\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "fig = plt.figure()\n",
        "for i in range(10):\n",
        "    plt.subplot(2, 5, i+1)\n",
        "    x_y = X_train[y_train == i]\n",
        "    plt.imshow(x_y[0], cmap='gray', interpolation='none')\n",
        "    plt.title(\"Class %d\" % (i))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    \n",
        "plt.tight_layout()\n"
      ],
      "metadata": {
        "id": "c5Gag2Z-r6hJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('X_train.shape', X_train.shape)\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
        "    X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)\n",
        "    input_shape = (1, 28, 28)\n",
        "else:\n",
        "    X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
        "    X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
        "    input_shape = (28, 28, 1)\n",
        "\n",
        "# the generator is using tanh activation, for which we need to preprocess \n",
        "# the image data into the range between -1 and 1.\n",
        "\n",
        "X_train = np.float32(X_train)\n",
        "X_train = (X_train / 255 - 0.5) * 2\n",
        "X_train = np.clip(X_train, -1, 1)\n",
        "\n",
        "print('X_train reshape:', X_train.shape)"
      ],
      "metadata": {
        "id": "n4KlA4adsGS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# latent space dimension\n",
        "latent_dim = 100\n",
        "\n",
        "# imagem dimension 28x28x1\n",
        "img_dim = 784\n",
        "\n",
        "init = initializers.RandomNormal(stddev=0.02)\n",
        "\n",
        "# Generator network\n",
        "generator = Sequential()\n",
        "\n",
        "# FC\n",
        "generator.add(Dense(7*7*512, input_shape=(latent_dim,), kernel_initializer=init))\n",
        "# generator.add(ReLU())\n",
        "generator.add(Reshape((7, 7, 512)))\n",
        "\n",
        "# # Conv 1\n",
        "generator.add(Conv2DTranspose(128, kernel_size=3, strides=2, padding='same'))\n",
        "generator.add(BatchNormalization(momentum=0.8))\n",
        "generator.add(ReLU(0.2))\n",
        "\n",
        "# Conv 2\n",
        "generator.add(Conv2DTranspose(64, kernel_size=3, strides=1, padding='same'))\n",
        "generator.add(BatchNormalization(momentum=0.8))\n",
        "generator.add(ReLU(0.2))\n",
        "\n",
        "# Output\n",
        "generator.add(Conv2DTranspose(1, kernel_size=3, strides=2, padding='same',\n",
        "                              activation='tanh'))"
      ],
      "metadata": {
        "id": "Axe6SVSiswg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Critic network\n",
        "critic = Sequential()\n",
        "\n",
        "# imagem shape 28x28x1\n",
        "img_shape = X_train[0].shape\n",
        "\n",
        "# Conv 1\n",
        "critic.add(Conv2D(64, kernel_size=3, strides=2, padding='same',\n",
        "                  input_shape=(img_shape)))\n",
        "critic.add(LeakyReLU(0.2))\n",
        "\n",
        "# Conv 2\n",
        "critic.add(Conv2D(128, kernel_size=3, strides=2, padding='same'))\n",
        "critic.add(BatchNormalization(momentum=0.8))\n",
        "critic.add(LeakyReLU(0.2))\n",
        "\n",
        "# Conv 3\n",
        "critic.add(Conv2D(256, kernel_size=3, strides=2, padding='same'))\n",
        "critic.add(BatchNormalization(momentum=0.8))\n",
        "critic.add(LeakyReLU(0.2))\n",
        "\n",
        "# Conv 4\n",
        "critic.add(Conv2D(512, kernel_size=3, strides=1, padding='same'))\n",
        "critic.add(BatchNormalization(momentum=0.8))\n",
        "critic.add(LeakyReLU(0.2))\n",
        "\n",
        "# FC\n",
        "critic.add(Flatten())\n",
        "\n",
        "# Output\n",
        "critic.add(Dense(1))"
      ],
      "metadata": {
        "id": "bomnoAUdtJ-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wasserstein objective\n",
        "def wasserstein_loss(y_true, y_pred):\n",
        "    return K.mean(y_true * y_pred)\n",
        "\n",
        "n_critic = 5\n",
        "clip_value = 0.01\n",
        "optimizer = RMSprop(lr=0.00005)\n",
        "\n",
        "critic.compile(optimizer=optimizer, loss=wasserstein_loss, metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "gDVqyRZKtY6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "batch_size = 64\n",
        "\n",
        "real = -np.ones(shape=(batch_size, 1))\n",
        "fake = np.ones(shape=(batch_size, 1))\n",
        "\n",
        "d_loss = []\n",
        "g_loss = []\n",
        "\n",
        "for e in range(epochs + 1):\n",
        "    for i in range(len(X_train) // batch_size):\n",
        "        for _ in range(n_critic):\n",
        "\n",
        "            # Train Discriminator weights\n",
        "            critic.trainable = True\n",
        "\n",
        "            # Real samples\n",
        "            X_batch = X_train[i*batch_size:(i+1)*batch_size]\n",
        "            d_loss_real = critic.train_on_batch(x=X_batch, y=real)\n",
        "\n",
        "            # Fake Samples\n",
        "            z = np.random.normal(loc=0, scale=1, size=(batch_size, latent_dim))\n",
        "            X_fake = generator.predict(z)\n",
        "            d_loss_fake = critic.train_on_batch(x=X_fake, y=fake)\n",
        "\n",
        "            # Discriminator loss\n",
        "            d_loss_batch = 0.5 * (d_loss_real[0] + d_loss_fake[0])\n",
        "\n",
        "            # Clip critic weights\n",
        "            for l in critic.layers:\n",
        "                weights = l.get_weights()\n",
        "                weights = [np.clip(w, -clip_value, clip_value) for w in weights]\n",
        "                l.set_weights(weights)\n",
        "\n",
        "        # Train Generator weights\n",
        "        critic.trainable = False\n",
        "        g_loss_batch = c_g.train_on_batch(x=z, y=real)\n",
        "\n",
        "        print(\n",
        "            'epoch = %d/%d, batch = %d/%d, d_loss=%.3f, g_loss=%.3f' % (e + 1, epochs, i, len(X_train) // batch_size, d_loss_batch, g_loss_batch[0]),\n",
        "            100*' ',\n",
        "            end='\\r'\n",
        "        )\n",
        "    \n",
        "    d_loss.append(d_loss_batch)\n",
        "    g_loss.append(g_loss_batch[0])\n",
        "    print('epoch = %d/%d, d_loss=%.3f, g_loss=%.3f' % (e + 1, epochs, d_loss[-1], g_loss[-1]), 100*' ')\n",
        "\n",
        "    if e % 10 == 0:\n",
        "        samples = 10\n",
        "        x_fake = generator.predict(np.random.normal(loc=0, scale=1, size=(samples, latent_dim)))\n",
        "\n",
        "        for k in range(samples):\n",
        "            plt.subplot(2, 5, k+1)\n",
        "            plt.imshow(x_fake[k].reshape(28, 28), cmap='gray')\n",
        "            plt.xticks([])\n",
        "            plt.yticks([])\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "QOQXVctKuKpD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}