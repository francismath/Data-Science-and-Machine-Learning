{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Train GAN using MNIST"
      ],
      "metadata": {
        "id": "D2dJG2BUppCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txVNlV1FjWlz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "import keras.backend as K\n",
        "from keras.layers import Input, Dense, Activation, LeakyReLU, BatchNormalization\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "plt.figure(figsize=(5, 4))\n",
        "for i in range(20):\n",
        "    plt.subplot(4, 5, i+1)\n",
        "    plt.imshow(X_train[i], cmap='gray')\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_latent_samples(n_samples, sample_size):\n",
        "    #return np.random.uniform(-1, 1, size=(n_samples, sample_size))\n",
        "    return np.random.normal(loc=0, scale=1, size=(n_samples, sample_size))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AOgIDOpykgGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_simple_GAN(sample_size, \n",
        "                    g_hidden_size, \n",
        "                    d_hidden_size, \n",
        "                    leaky_alpha, \n",
        "                    g_learning_rate,\n",
        "                    d_learning_rate):\n",
        "    K.clear_session()\n",
        "    \n",
        "    generator = Sequential([\n",
        "        Dense(g_hidden_size, input_shape=(sample_size,)),\n",
        "        LeakyReLU(alpha=leaky_alpha),\n",
        "        Dense(784),        \n",
        "        Activation('tanh')\n",
        "    ], name='generator')    \n",
        "\n",
        "    discriminator = Sequential([\n",
        "        Dense(d_hidden_size, input_shape=(784,)),\n",
        "        LeakyReLU(alpha=leaky_alpha),\n",
        "        Dense(1),\n",
        "        Activation('sigmoid')\n",
        "    ], name='discriminator')    \n",
        "    \n",
        "    gan = Sequential([\n",
        "        generator,\n",
        "        discriminator\n",
        "    ])\n",
        "    \n",
        "    discriminator.compile(optimizer=Adam(lr=d_learning_rate), loss='binary_crossentropy')\n",
        "    gan.compile(optimizer=Adam(lr=g_learning_rate), loss='binary_crossentropy')\n",
        "    \n",
        "    return gan, generator, discriminator"
      ],
      "metadata": {
        "id": "WAb8cW6ymayp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(x):    \n",
        "    x = x.reshape(-1, 784) # 784=28*28\n",
        "    x = np.float64(x)\n",
        "    x = (x / 255 - 0.5) * 2\n",
        "    x = np.clip(x, -1, 1)\n",
        "    return x\n",
        "X_train_real = preprocess(X_train)\n",
        "X_test_real  = preprocess(X_test)\n",
        "\n",
        "def deprocess(x):\n",
        "    x = (x / 2 + 1) * 255\n",
        "    x = np.clip(x, 0, 255)\n",
        "    x = np.uint8(x)\n",
        "    x = x.reshape(28, 28)\n",
        "    return x\n",
        "\n",
        "def make_labels(size):\n",
        "    return np.ones([size, 1]), np.zeros([size, 1])\n"
      ],
      "metadata": {
        "id": "vpaIvgnOnOoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "sample_size     = 100     # latent sample size (i.e., 100 random numbers)\n",
        "g_hidden_size   = 128\n",
        "d_hidden_size   = 128\n",
        "leaky_alpha     = 0.01\n",
        "g_learning_rate = 0.0001  # learning rate for the generator\n",
        "d_learning_rate = 0.001   # learning rate for the discriminator\n",
        "epochs          = 100\n",
        "batch_size      = 64      # train batch size\n",
        "eval_size       = 16      # evaluate size\n",
        "smooth          = 0.1\n",
        "\n",
        "# labels for the batch size and the test size\n",
        "y_train_real, y_train_fake = make_labels(batch_size)\n",
        "y_eval_real,  y_eval_fake  = make_labels(eval_size)\n",
        "\n",
        "# create a GAN, a generator and a discriminator\n",
        "gan, generator, discriminator = make_simple_GAN(\n",
        "    sample_size, \n",
        "    g_hidden_size, \n",
        "    d_hidden_size, \n",
        "    leaky_alpha, \n",
        "    g_learning_rate,\n",
        "    d_learning_rate)\n",
        "\n",
        "losses = []\n",
        "for e in range(epochs):\n",
        "    for i in range(len(X_train_real)//batch_size):\n",
        "        # real MNIST digit images\n",
        "        X_batch_real = X_train_real[i*batch_size:(i+1)*batch_size]\n",
        "        \n",
        "        # latent samples and the generated digit images\n",
        "        latent_samples = make_latent_samples(batch_size, sample_size)\n",
        "        X_batch_fake = generator.predict_on_batch(latent_samples)\n",
        "        \n",
        "        # train the discriminator to detect real and fake images\n",
        "        make_trainable(discriminator, True)\n",
        "        discriminator.train_on_batch(X_batch_real, y_train_real * (1 - smooth))\n",
        "        discriminator.train_on_batch(X_batch_fake, y_train_fake)\n",
        "\n",
        "        # train the generator via GAN\n",
        "        make_trainable(discriminator, False)\n",
        "        gan.train_on_batch(latent_samples, y_train_real)\n",
        "    \n",
        "    # evaluate\n",
        "    X_eval_real = X_test_real[np.random.choice(len(X_test_real), eval_size, replace=False)]\n",
        "    \n",
        "    latent_samples = make_latent_samples(eval_size, sample_size)\n",
        "    X_eval_fake = generator.predict_on_batch(latent_samples)\n",
        "\n",
        "    d_loss  = discriminator.test_on_batch(X_eval_real, y_eval_real)\n",
        "    d_loss += discriminator.test_on_batch(X_eval_fake, y_eval_fake)\n",
        "    g_loss  = gan.test_on_batch(latent_samples, y_eval_real) # we want the fake to be realistic!\n",
        "    \n",
        "    losses.append((d_loss, g_loss))\n",
        "    \n",
        "    print(\"Epoch: {:>3}/{} Discriminator Loss: {:>6.4f} Generator Loss: {:>6.4f}\".format(\n",
        "        e+1, epochs, d_loss, g_loss))\n",
        "    "
      ],
      "metadata": {
        "id": "DzPbqpr_npbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_samples = make_latent_samples(20, sample_size)\n",
        "generated_digits = generator.predict(latent_samples)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "for i in range(20):\n",
        "    img = deprocess(generated_digits[i])\n",
        "    plt.subplot(4, 5, i+1)\n",
        "    plt.imshow(img, cmap='gray')\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uVtb92v6oV4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train GAN using CIFAR10\n"
      ],
      "metadata": {
        "id": "ZfvS6rYnpxQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O-j3jzMsp6rJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}